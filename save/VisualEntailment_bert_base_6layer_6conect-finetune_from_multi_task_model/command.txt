Namespace(baseline=False, bert_model='bert-base-uncased', clean_train_sets=True, config_file='config/bert_base_6layer_6conect.json', do_lower_case=True, dynamic_attention=False, evaluation_interval=1, fp16=False, freeze=-1, from_pretrained='multi_task_model.bin', gradient_accumulation_steps=1, in_memory=False, local_rank=-1, loss_scale=0, lr_scheduler='warmup_linear', no_cuda=False, num_train_epochs=20, num_workers=16, optim='AdamW', output_dir='save', resume_file='', save_name='finetune_from_multi_task_model', seed=0, task_specific_tokens=True, tasks='13', train_iter_gap=4, train_iter_multiplier=1.0, vision_scratch=False, visual_target=0, warmup_proportion=0.1)


{
  "attention_probs_dropout_prob": 0.1,
  "bi_attention_type": 1,
  "bi_hidden_size": 1024,
  "bi_intermediate_size": 1024,
  "bi_num_attention_heads": 8,
  "dynamic_attention": false,
  "fast_mode": false,
  "fixed_t_layer": 0,
  "fixed_v_layer": 0,
  "fusion_method": "mul",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "in_batch_pairs": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "model": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_negative": 128,
  "objective": 0,
  "pooling_method": "mul",
  "t_biattention_id": [
    6,
    7,
    8,
    9,
    10,
    11
  ],
  "task_specific_tokens": false,
  "type_vocab_size": 2,
  "v_attention_probs_dropout_prob": 0.1,
  "v_biattention_id": [
    0,
    1,
    2,
    3,
    4,
    5
  ],
  "v_feature_size": 2048,
  "v_hidden_act": "gelu",
  "v_hidden_dropout_prob": 0.1,
  "v_hidden_size": 1024,
  "v_initializer_range": 0.02,
  "v_intermediate_size": 1024,
  "v_num_attention_heads": 8,
  "v_num_hidden_layers": 6,
  "v_target_size": 1601,
  "visual_target": 0,
  "visualization": false,
  "vocab_size": 30522,
  "with_coattention": true
}

